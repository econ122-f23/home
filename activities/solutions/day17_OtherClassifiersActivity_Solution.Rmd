---
title: "Other Classifiers Activity - Solution" 
author: "ECON 122"
date: "Day 17"
output: 
  github_document:
    pandoc_args: --webtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse=TRUE, prompt=TRUE,comment=NULL,message=FALSE, include=TRUE, fig.width = 9, fig.height = 4)
```

```{r packageCheck, include=FALSE}
mypacks <- c("ggplot2","dplyr","readr","tidyr", "ROCR", "boot","class","randomForest","e1071", "stringr","partykit","rpart")  # what packages are needed?
packs <- installed.packages()   # find installed package list
install.me <- mypacks[!(mypacks %in% packs[,"Package"])]  #what needs to be installed?
if (length(install.me) >= 1) install.packages(install.me, repos = "http://cran.us.r-project.org")   # install (if needed)
lapply(mypacks, library, character.only=TRUE)  # load all packages
```

## Example 1: Spam using k-nn

This example looks at a data set of about 4600 emails that are classified as spam or not spam, along with over 50 variables measuring different characteristic of the email. Details about these variables are found on the  [Spambase example ](http://archive.ics.uci.edu/ml/datasets/Spambase) on the machine learning data archive. The dataset linked to below is a slightly cleaned up version of this data. The only extra column in the data is `rgroup` which is a randomly assigned grouping variable (groups 0 through 99).

Read the data in
```{r}
# tsv = tab separated values!
spam <- read_delim("https://raw.githubusercontent.com/mgelman/data/master/spamD.txt", delim="\t", col_types = cols(spam = col_factor(levels=c("non-spam","spam")), .default = col_double()))
```

Here we fit a k-nn classifier using the 57 quantitative predictors from the spam data to an 80%/20% training and test set split. We used $k=10$.
```{r}
set.seed(7)
n <- nrow(spam)
train_index <- sample(1:n, size=round(.8*n))
trainX <- spam %>% slice(train_index) %>% select(-rgroup, -spam)
testX <- spam %>% slice(-train_index) %>% select(-rgroup, -spam)
spam_knn1 <- knn(trainX, testX, cl= spam$spam[train_index], k=10)
```

#### Question 1
Compute the accuracy, error rate and recall for the 20% test set. 

#### *Answer:*

```{r answerQ1}
(conf.mat <- table(spam$spam[-train_index], spam_knn1))
sum(diag(prop.table(conf.mat))) 
1-sum(diag(prop.table(conf.mat))) 
prop.table(conf.mat,1) 
```

For the k-nn method with $k=10$: the accuracy for predicting the test set is `r round(100*sum(diag(prop.table(conf.mat))),1)`%, the error rate is  `r round(100-100*sum(diag(prop.table(conf.mat))),1)`%, and the recall detecting spam is `r round(100*prop.table(conf.mat,1)[2,2],1)`%.

#### Question 2
Any statistical method that uses a distance metric can yield results that are sensitive to the scale of the variables. In the k-nn we are using a distance measure with the predictors. Use an `apply` command to compute the sd of the 57 predictors. Are they similar or different in value?

#### *Answer:*
No, the histogram of sd's for all the predictor variables show a wide range. A closer look at these variables shows that anything that is a frequency value (# of this type out of total number) has a fairly small scale sd while the run length variables have large SDs. 

```{r answerQ2}
sds <- apply(spam[,1:57], 2, sd)
hist(sds)
head(sds)
tail(sds)
```

#### Question 3
We can use the `scale` function on a data frame to standardize each column of the test and training sets. Verify that the sd of the scaled training and test sets are now all equal to 1. 
```{r, include=FALSE}
trainX <- scale(trainX)
testX <- scale(testX)
```

#### *Answer:*
The min and max of both sets are equal to 1.  

```{r answerQ3}
sds_train <- apply(trainX, 2, sd)
sds_test <- apply(testX, 2, sd)
summary(sds_train)
summary(sds_test)
```


#### Question 4
Refit the k-nn classifier to the scaled predictor sets using $k=10$. How have accuracy, error and recall rates changed? 


#### *Answer:*

```{r answerQ4}
spam_knn2 <- knn(trainX, testX, cl= spam$spam[train_index], k=10)
(conf.mat <- table(spam$spam[-train_index], spam_knn2))
sum(diag(prop.table(conf.mat))) 
1-sum(diag(prop.table(conf.mat))) 
prop.table(conf.mat,1) 
```

For the k-nn method with $k=10$ and standardized predictors: the accuracy for predicting the test set is `r round(100*sum(diag(prop.table(conf.mat))),1)`%, the error rate is  `r round(100-100*sum(diag(prop.table(conf.mat))),1)`%, and the recall detecting spam is `r round(100*prop.table(conf.mat,1)[2,2],1)`%.

#### Question 5
Refit the k-nn classifier using the scaled predictors from question 4, but this time let `k` vary from 1 to 30. Write a function that returns the Error/accuracy and recall rates for any value of k, then use an `sapply` command to get the rates for the kâ€™s up to 30. Which value of k looks optimal for this data?

#### *Answer:*
The function just needs a data frame of predictors variables and the responses for these predictors. It uses LOOCV to predict the response for each case (e.g. each case is a single test set) and it returns a vector of classifications for each case in the data:
```{r, cache=TRUE}
k <- seq(1,50,by=3)
scaledX <- scale(spam[,1:57])  # scale x variables
knn_fn <- function(k)
{
  data_frame(k=k, prediction = knn.cv(scaledX, cl= spam$spam, k=k), y=spam$spam)
}
preds <- lapply(k, knn_fn)
preds_df <- bind_rows(preds)
stats.cv.scale<- preds_df %>% group_by(k) %>% 
  summarize(accuracy = mean(prediction == y), 
                   recall = sum(prediction == "spam" & y == "spam")/sum(y == "spam"),
                    predictors = "scaled") 
stats.cv.scale %>%  gather(key=stat, value = value, 2:3) %>%
  ggplot(aes(x=k, y=value, color=stat))  + 
    geom_point() + geom_line() + 
  ggtitle("LOOCV")
```

It looks like k's between 5 and 20 yeild an error rate just under 10%, but because recall drops a lot in this range. I would pick either 1 (the highest `accuracy` and `recall` rates) or a `k` between 5 and 10 if I'm worried a `k` might over-fit too much 


## Example 2: Spam using tree methods
Let's now consider tree-based methods for classifying spam. Let's go back to the original scale of the predictor variables (and incldue spam in the training/test data frames):
```{r}
set.seed(7)
n <- nrow(spam)
train_index <- sample(1:n, size=round(.8*n))
train <- spam %>% slice(train_index) %>% select(-rgroup)
test <- spam %>% slice(-train_index) %>% select(-rgroup)
```
We will also steal the formula from the logistic model from day 18:
```{r}
xvars <- str_c(names(spam)[1:57], collapse="+")
myform <- as.formula(str_c("spam ~ ", xvars))
myform
```

#### Question 6
Fit a decision tree to training data then compute the accuracy, error and recall for the test data. What predictors look to be the important predictors of spam based on this tree?

#### *Answer:*

```{r answerQ6}
spam_dtree <- rpart(myform, data=train)
spam_dtree
plot(as.party(spam_dtree),gp = gpar(fontsize = 8))
test <- test %>% mutate(pred_dtree = predict(spam_dtree, newdata=test, type="class"))
(conf.mat <- table(test$spam, test$pred_dtree))
sum(diag(prop.table(conf.mat))) 
1-sum(diag(prop.table(conf.mat))) 
prop.table(conf.mat,1) 
```

For a decision tree: the accuracy for predicting the test set is `r round(100*sum(diag(prop.table(conf.mat))),1)`%, the error rate is  `r round(100-100*sum(diag(prop.table(conf.mat))),1)`%, and the recall detecting spam is `r round(100*prop.table(conf.mat,1)[2,2],1)`%. Some of the important predictors look to be the frequency of $ (more $ signs means more spam), ! (bang) (fewer ! means less spam), the frequency of the word remove (more instances means more spam), and the frequency of the word free (more free means more spam). 

#### Question 7
Fit a random forest model to the spam data using the `randomForest` command. Use the default settings (with produce 500 trees with $m \approx \sqrt{p}$). Compute the accuracy, error and recall for the test data. What predictors look to be the important predictors of spam based on this method? Are they similar to the variable found in question 6?

#### *Answer:*

```{r answerQ7, fig.height=6}
spam_forest <- randomForest(myform, data=train)
spam_forest
test <- test %>% mutate(pred_forest = predict(spam_forest, newdata=test, type="class"))
( conf.mat <- table(test$spam, test$pred_forest))
varImpPlot(spam_forest)
sum(diag(prop.table(conf.mat))) 
1-sum(diag(prop.table(conf.mat))) 
prop.table(conf.mat,1) 
```

For a random forest classifier: the accuracy for predicting the test set is `r round(100*sum(diag(prop.table(conf.mat))),1)`%, the error rate is  `r round(100-100*sum(diag(prop.table(conf.mat))),1)`%, and the recall detecting spam is `r round(100*prop.table(conf.mat,1)[2,2],1)`%. Similar predictors to those found in Q6 look important, like frequency of dollar and "bang" characters and remove frequency.

**Note**: Having a high importance itself doesn't tell us whether that variable is positively or negatively associated with the outcome. In this case, the examples are provided by "George" from "HP" so emails with those words are likely **NOT** spam. 

